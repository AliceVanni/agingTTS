# Aging TTS
This repository collects all the documents and programs for the implementation of the text-to-speech system I developed as part of my Thesis Project for the MSc Voice Technology.

The implementation uses FastSpeech2 based on [ming024 PyTorch implementation](https://github.com/ming024/FastSpeech2). The system was inpired by the pipeline developed for [ChildTTS](https://github.com/C3Imaging/ChildTTS), which in turn is based on [CorentinJ code](https://github.com/CorentinJ/Real-Time-Voice-Cloning).

The main difference from the basic FastSpeech2 model is the addition of age embeddings in order to model and control the age of the synthesised voice.

## Quickstart
The model was trained and developed using Hábrók, the High-Performance Computer cluster of the University of Groningen.
### Setting of the environment
The whole model was implemented using a python's virtual enviroment.
To install the dependencies:
```ruby
pip3 install -r requirements.txt
```
### Inference and controllability

#### HiFi-GAN vocoder
First you need to unzip the checkpoints for the HiFi-GAN vocoder.
```ruby
unzip hifigan/generator_universal.pth.tar.zip -d hifigan
```

### Pretrained models
The current repo only have one model available, CV_17_Delta.
To run inference on this model run
```ruby
python3 synthesize.py --text "YOUR_DESIRED_TEXT" --speaker_id <speaker_id> --restore_step 40500 --mode single -p config/CV_17_Delta/preprocess.yaml -m config/CV_17_Delta/model.yaml -t config/CV_17_Delta/train.yaml
```

The speech can be generated by controlling duration, pitch, energy and age with these parameters:

- --duration_control <scalar>
- --energy_control <scalar>
- --pitch_control <scalar>
- --age_control <string> → options: child, adult, senior

#### Preprocessing
For the preprocessing, run
```ruby
python3 preprocess.py config/CV_17_Delta/preprocess.yaml
```
A warning will appear since the audio files used are .mp3 format.
The warning is caused by the use of PySoundFile, which cannot handle .mp3 files.
This will only mean that the code will run slower by using audioread.

To start the trainning, run
```ruby
python3 train.py -p config/CV_17_Delta/preprocess.yaml -m config/CV_17_Delta/model.yaml -t config/CV_17_Delta/train.yaml
```

## Instructions and steps to train the model from scratch 
Below you can find a step by step guide to train this TTS model from scratch (i.e. with a new dataset).
First of all you want to clone the current repository and install the dependencies as described in the paragraph above. 

### Dataset preparation
The dataset you want to use for training needs to have a specific structure, following the style of Prosodylab-aligner:
```
+-- main_corpus_directory
|   +-- speaker1
|	--- recording1.wav
|	--- recording1.lab
|	--- recording2.wav
|	--- recording2.lab
|   +-- speaker2
|	--- recording3.wav
|	--- recording3.lab
|	--- ...
```

I have provided some code to generate some useful text files and a directory with a subset of data with specific requirements:
- Available information about the gender of the speaker;
- Available information about the age of the speaker, or the age group to which they belongs;
- Sample balance on gender and age;

The following instructions are valid for CommonVoice datasets, for other dataset, you might need to adjust the code.
```ruby
python3 dataset_preparation.py
```
This code uses functions defined in utils/dataset.py to prepare all the necessary files and get the relevant information from the original dataset files.
For the training you will also need .lab files, which contain the transcription of each audio file. 
To generate them you can run
```ruby
python3 lab_file_generation.py
```
This code uses the function defined in utils/generate_lab_files.py to generate the lab files for the training. The generation of such files is based on the files output by the previous code.

In case the TextGrids for your dataset are not available, you can obtain them by training Montreal Forced Aligner (MFA).

### MFA training
Download the pretrained dictionary, acoustic model and g2p model from MFA website by running
```ruby
mfa model download dictionary <your_language_mfa>
mfa model download acoustic <your_language_mfa>
mfa model download g2p <your_language_mfa>
```
Then, validate your datatset:
```ruby
mfa validate corpus_prepared/ <your_lang_dictionary_mfa> <your_lang_acoustic_mfa>
```

In case many OOV words are found, you might want to run the g2p to improve the model's performance later
```ruby
mfa g2p --dictionary_path /your_path/your_lang_dictionary_mfa.dict corpus_prepared/ <your_lang_g2p_mfa> output_file.txt
```
Then, add the output to the dictionary by copy-pasting it in the txt file (alphabetical order is not important).
Validate again to check for improvements on OOVs.

Once you're done, you can generate the textgrids, which then have to be saved in preprocessed_data/corpus in a folder named *TextGrid*.
```ruby
mfa align corpus_prepared/ <your_lang_dictionary_mfa> <your_lang_acoustic_mfa> <output_folder> ...
```
Before moving on, it is important to make sure that the TextGrids are correctly generated.

### Prepare the config files
In the config folder, there should be a folder dedicated to the dataset you want to use. In this subfolder there should be three yaml files:
- model.yaml
- preprocess.yaml
- train.yaml

The first defines the *model architecture* (e.g. number of layers per module, etc). 
There should be no need to modify it from the original, unless you want to use a different layout.

The second file contains the information to *generate the necessary files for training*, i.e. the ones in the preprocessed_data folder.
This preprocessing script looks for the audio data at the path specified in the raw_path variable, which is the corpus directory uploaded above (one folder per speaker, audio files together with the corresponding .lab files).

The train.yaml script contains the necessary information for the *actual training*, including the folder in which to save the output and the training parameters (e.g. batch_size, anneal step, total number of steps, etc.).

### Preprocessing
The preprocessing command is the same specified above. Since the CV audio files are .mp3, I added a get_mel_from_mp3() in audio/tools.py and I changed every occurrence of wav to mp3 in preprocessor.py.
In case use are using .wav files, you will need to adjust the code.
  	*N.b. The preprocessing has to be done with a GPU available.*

### Training
Before training, the symbols.py file should be adjusted. 
The function to extract all the single values in the dictionary is defined in text/get_unique_characters.py. 
To update the symbols.py file you should run
```ruby
python3 generate_symbols.py
```
This code is specifically made for IPA symbols, but since it just extracts all the unique symbols used in the transcription it will work fine with any other type of alphabet (theoretically).
To start the training you just have to run the command specified in the section above.

The saving step is set to 1500, but it can be changed in the config/train.yaml file, as well as the total number of steps, which is set to the default value of 900000 at the moment.

### Inference
To test your model run inference as described above.


Python version used: Python 3.9.6
