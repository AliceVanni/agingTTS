# Aging TTS
This repository collects all the documents and programs for the implementation of the text-to-speech system I developed as part of my Thesis Project for the MSc Voice Technology.

The implementation uses FastSpeech2 based on [ming024 PyTorch implementation](https://github.com/ming024/FastSpeech2). The system was inpired by the pipeline developed for [ChildTTS](https://github.com/C3Imaging/ChildTTS), which in turn is based on [CorentinJ code](https://github.com/CorentinJ/Real-Time-Voice-Cloning).

The main differences from the basic FastSpeech2 model are: 
- the employment of [Resemblyzer](https://github.com/resemble-ai/Resemblyzer) voice encoder by to model speaker embeddings;
- the addition of age embeddings in order to model and control the age of the synthesised voice.

## Quickstart
The model was trained and developed using Hábrók, the High-Performance Computer cluster of the University of Groningen.
### Setting of the environment
The whole model was implemented using a python's virtual enviroment using Python 3.6.8.
To install the dependencies:
```ruby
pip3 install -r requirements.txt
```
### Inference and controllability
The current repo two models available, agingTTS_22kHz and agingTTS_22kHz_bn.
To run inference you can download the [pretrained model on English](https://drive.google.com/drive/folders/1y3pbT9dsetuu0QJkT-uecR8Q7Ax3nGdq?usp=drive_link) and run:
```ruby
python3 synthesize.py --text "TARGET_TEXT" --speaker_id SPEAKER_ID --restore_step CKPT_NUMBER --mode single -p config/model_name/preprocess.yaml -m config/model_name/model.yaml -t config/model_name/train.yaml
```

To test the inference using the validation set, you can run batch inference by replacing the ```--text "TARGET_TEXT"``` parameter with ```--source preprocessed_data/model_name/val.txt```

As in [ming024's implememtation](https://github.com/ming024/FastSpeech2), the speech can be generated by controlling pitch, volume and speaking rate of the synthesised utterances by specifying the desired pitch, energy, duration ratios respectively. 
Additionally, an age parameter is added through which it is possible to control the perceived age of the synthesised voice.
The specific parameters are:

- --duration_control <scalar>
- --energy_control <scalar>
- --pitch_control <scalar>
- --age_control <string> → options*: child, adult, senior

  *These options only refer to the present implementation. The model is flexible and can model different age categories too.

#### HiFi-GAN vocoder
First you need to unzip the checkpoints for the HiFi-GAN vocoder.
```ruby
unzip hifigan/generator_universal.pth.tar.zip -d hifigan
```

### Pretrained models
The current repo two models available, agingTTS and agingTTS-BN.
To run inference on this model run
```ruby
python3 synthesize.py --text "YOUR_DESIRED_TEXT" --speaker_id SPEAKER_ID --restore_step CKPT_NUMBER --mode single -p config/model_name/preprocess.yaml -m config/model_name/model.yaml -t config/model_name/train.yaml
```

#### Preprocessing
For the preprocessing, run
```ruby
python3 preprocess.py config/corpus_name/preprocess.yaml
```
A warning will appear since the audio files used are .mp3 format.
The warning is caused by the use of PySoundFile, which cannot handle .mp3 files.
This will only mean that the code will run slower by using audioread.

To start the trainning, run
```ruby
python3 train.py -p config/corpus_name/preprocess.yaml -m config/corpus_name/model.yaml -t config/corpus_name/train.yaml
```

## Instructions and steps to train the model from scratch 
Below you can find a step by step guide to train this TTS model from scratch (i.e. with a new dataset).
First of all you want to clone the current repository and install the dependencies as described in the paragraph above. 

### Dataset preparation
The dataset you want to use for training needs to have a specific structure, following the style of Prosodylab-aligner:
```
+-- main_corpus_directory
|   +-- speaker1
|	--- recording1.wav
|	--- recording1.lab
|	--- recording2.wav
|	--- recording2.lab
|   +-- speaker2
|	--- recording3.wav
|	--- recording3.lab
|	--- ...
```

I have provided some code to generate some useful text files and a directory with a subset of data with specific requirements:
- Available information about the gender of the speaker;
- Available information about the age of the speaker, or the age group to which they belongs;
- Sample balance on gender and age;

The following instructions are valid for CommonVoice datasets, for other dataset, you might need to adjust the code.
```ruby
python3 dataset_preparation.py
```
This code uses functions defined in utils/dataset.py to prepare all the necessary files and get the relevant information from the original dataset files.
For the training you will also need .lab files, which contain the transcription of each audio file. 
To generate them you can run
```ruby
python3 lab_file_generation.py
```
This code uses the function defined in utils/generate_lab_files.py to generate the lab files for the training. The generation of such files is based on the files output by the previous code.

### Prepare the config files
In the config folder, there should be a folder dedicated to the dataset you want to use. In this subfolder there should be three yaml files:
- model.yaml
- preprocess.yaml
- train.yaml

The first defines the *model architecture* (e.g. number of layers per module, etc). 
There should be no need to modify it from the original, unless you want to use a different layout.

The second file contains the information to *generate the necessary files for training*, i.e. the ones in the preprocessed_data folder.
This preprocessing script looks for the audio data at the path specified in the raw_path variable, which is the corpus directory uploaded above (one folder per speaker, audio files together with the corresponding .lab files).

The train.yaml script contains the necessary information for the *actual training*, including the folder in which to save the output and the training parameters (e.g. batch_size, anneal step, total number of steps, etc.).

### Preprocessing
The preprocessing prepares the data to be used by the network for training.
In case the TextGrids for your dataset are not available, you can obtain them by training Montreal Forced Aligner (MFA).

#### MFA training
Download the pretrained dictionary, acoustic model and g2p model from MFA website by running
```ruby
mfa model download dictionary <your_language_mfa>
mfa model download acoustic <your_language_mfa>
mfa model download g2p <your_language_mfa>
```
Then, validate your datatset:
```ruby
mfa validate corpus_prepared/ <your_lang_dictionary_mfa> <your_lang_acoustic_mfa>
```

In case many OOV words are found, you might want to run the g2p to improve the model's performance later
```ruby
mfa g2p --dictionary_path /your_path/your_lang_dictionary_mfa.dict corpus_prepared/ <your_lang_g2p_mfa> output_file.txt
```
Then, add the output to the dictionary by copy-pasting it in the txt file (alphabetical order is not important).
Validate again to check for improvements on OOVs.

Once you're done, you can generate the textgrids, which then have to be saved in preprocessed_data/corpus in a folder named *TextGrid*.
```ruby
mfa align corpus_prepared/ <your_lang_dictionary_mfa> <your_lang_acoustic_mfa> <output_folder> ...
```
Before moving on, it is important to make sure that the TextGrids are correctly generated.


Once the alignment is correctly done, you can run the preprocessing script in the config file previously prepared
```ruby
python3 preprocess.py config/<corpus_name>/preprocess.yaml
```
_N.b. The preprocessing has to be done with a GPU available._

### Training
Before training, if you are working with a language other than English, the symbols.py file should be adjusted. 
The function to extract all the single values in the dictionary is defined in text/get_unique_characters.py. 
To update the symbols.py file you should run
```ruby
python3 generate_symbols.py
```
This code is specifically made for IPA symbols, but since it just extracts all the unique symbols used in the transcription it will work fine with any other type of alphabet (theoretically).

Before training the model, especially if your data includes more age groups than the one used in the present my case, you might want to pre-train the speaker embeddings with Resemblyzer. To do so, I advise you to create a new venv to avoid dependency conflicts with agingTTS environment. Then you can install resemblyzer following the indications [here](https://github.com/resemble-ai/Resemblyzer).
Once you're done, you can run:
```ruby
python3 model/resemblyzer_speaker_embeddings.py -c '<NAME_OF_YOUR_DATASET>'
```
This will save the pretrained speaker embeddings in the file named as specified in the model config file under 'pretrained_speaker_embeddings'.

To start the training, the command is:
```ruby
python3 train.py -p config/<corpus_name>/preprocess.yaml -m config/<corpus_name>/model.yaml -t config/<corpus_name>/train.yaml
```

The saving step is set to 10000, but it can be changed in the config/.../train.yaml file, as well as the total number of steps, which is set to the default value of 900000 at the moment.

### Inference
To test your model run inference as described in [Inference and Controllability section](https://github.com/AliceVanni/agingTTS/edit/main/README.md#inference-and-controllability).

## References
### Repositories
- [ming024's FastSpeech2 implementation](https://github.com/ming024/FastSpeech2)
- [ChildTTS implementation](https://github.com/C3Imaging/ChildTTS)
- [Resemblyzer implementation](https://github.com/resemble-ai/Resemblyzer)
### Articles and papers
- Chien, C.-M., Lin, J.-H., Huang, C.-y., Hsu, P.-c., & Lee, H.-y. (2021). [Investigating on incorporatingpretrained and learnable speaker representations for multi-speaker multi-style text-to-speech](https://ieeexplore.ieee.org/document/9413880). ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Pro-cessing (ICASSP), 8588–8592.
- Jain, R., Yiwere, M., Bigioi, D., Corcoran, P., & Cucu, H. (2022). [A Text-to-Speech Pipeline, Eval-uation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis](https://doi.org/10.48550/arXiv.2203.11562).
- Wan, L., Wang, Q., Papir, A., & Moreno, I. L. (2020). [Generalized End-to-End Loss for SpeakerVerification](https://arxiv.org/pdf/1710.10467).
